{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1dc2f4",
   "metadata": {
    "papermill": {
     "duration": 0.007521,
     "end_time": "2024-08-14T12:07:05.058645",
     "exception": false,
     "start_time": "2024-08-14T12:07:05.051124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **GitHub-Trending Data Collection**\n",
    "\n",
    "---\n",
    "\n",
    "Autor: **Gabriel Lino**\n",
    "\n",
    "<div style=\"display: flex; gap: 10px;\"> \n",
    "  <a href=\"mailto:gabriel.godoitb@gmail.com\"><img src=\"https://img.shields.io/badge/-Gmail-%23333?style=for-the-badge&logo=gmail&logoColor=white\" target=\"_blank\"></a>\n",
    "  <a href=\"https://www.linkedin.com/in/glgodoi\" target=\"_blank\"><img src=\"https://img.shields.io/badge/-LinkedIn-%230077B5?style=for-the-badge&logo=linkedin&logoColor=white\" target=\"_blank\"></a>  \n",
    "  <a href=\"https://github.com/GLK-7\" target=\"_blank\"><img src=\"https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white\" target=\"_blank\"></a>  \n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24535fcd",
   "metadata": {
    "papermill": {
     "duration": 0.006863,
     "end_time": "2024-08-14T12:07:05.072874",
     "exception": false,
     "start_time": "2024-08-14T12:07:05.066011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Tópicos**\n",
    "\n",
    "<ol type=\"1\">\n",
    "    <li>Contexto;</li>\n",
    "    <li>Pacotes e Bibliotecas;</li>\n",
    "    <li>Extração;</li>\n",
    "    <li>Manipulação.</li>\n",
    "    <li>Salvamento.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe6bf4",
   "metadata": {
    "papermill": {
     "duration": 0.006858,
     "end_time": "2024-08-14T12:07:05.087026",
     "exception": false,
     "start_time": "2024-08-14T12:07:05.080168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Contexto\n",
    "\n",
    "<div style=\"padding: 10px; flex-direction: column;\">\n",
    "    <div style=\"display: flex; align-items: center; flex-direction: row; margin-bottom: 20px;\">\n",
    "        <div style=\"display: inline-block; background-color: white; border-radius: 10px; padding: 20px;\">\n",
    "            <img width=\"150em\" height=\"auto\" src=\"https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/github/github-original-wordmark.svg\" alt=\"GitHub Logo\"/>\n",
    "        </div>\n",
    "        <p style=\"margin-left: 10px;\">\n",
    "            O <a href=\"https://github.com/\">GitHub</a> é uma plataforma de hospedagem de código-fonte e desenvolvimento colaborativo que permite que desenvolvedores ao redor do mundo compartilhem e colaborem em projetos. Ele se destaca como um dos maiores repositórios de código aberto, onde projetos populares são frequentemente destacados na seção \"Trending\", que mostra os repositórios mais relevantes e ativos em determinado período.\n",
    "        </p>\n",
    "    </div>\n",
    "    <p>\n",
    "    Neste projeto, realizaremos a coleta de dados da seção \"Trending\" do GitHub, utilizando técnicas de web crawling com Python. O objetivo é capturar informações sobre os repositórios em destaque, como nome, descrição, linguagem de programação utilizada, número de estrelas, entre outros. Os dados coletados serão organizados e salvos em um arquivo CSV, permitindo análises futuras sobre as tendências e popularidade de projetos no GitHub.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95394a",
   "metadata": {
    "papermill": {
     "duration": 0.007617,
     "end_time": "2024-08-14T12:07:05.102456",
     "exception": false,
     "start_time": "2024-08-14T12:07:05.094839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2\\. Pacotes e Bibliotecas\n",
    "\n",
    "Abaixo será feita a importação dos pacotes e bibliotecas utilizados nesse projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff26a8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T12:07:05.119338Z",
     "iopub.status.busy": "2024-08-14T12:07:05.118260Z",
     "iopub.status.idle": "2024-08-14T12:07:06.435437Z",
     "shell.execute_reply": "2024-08-14T12:07:06.434089Z"
    },
    "papermill": {
     "duration": 1.328816,
     "end_time": "2024-08-14T12:07:06.438497",
     "exception": false,
     "start_time": "2024-08-14T12:07:05.109681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importar Pacotes e Bibliotecas do Python\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from requests.exceptions import HTTPError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae282fd2",
   "metadata": {
    "papermill": {
     "duration": 0.006633,
     "end_time": "2024-08-14T12:07:06.452241",
     "exception": false,
     "start_time": "2024-08-14T12:07:06.445608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3\\. Extração"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3484dda8",
   "metadata": {
    "papermill": {
     "duration": 0.007086,
     "end_time": "2024-08-14T12:07:06.466381",
     "exception": false,
     "start_time": "2024-08-14T12:07:06.459295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Crawling\n",
    "\n",
    "Web crawling é uma técnica utilizada para coletar dados de sites automaticamente. Com Python, essa técnica envolve o uso de bibliotecas como `BeautifulSoup` e `requests` para acessar páginas da web, extrair informações relevantes e estruturar esses dados de forma organizada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfca9cc",
   "metadata": {
    "papermill": {
     "duration": 0.006608,
     "end_time": "2024-08-14T12:07:06.480114",
     "exception": false,
     "start_time": "2024-08-14T12:07:06.473506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "No código a seguir, primeiramente, é realizada a conexão com a página de \"Trending\" do GitHub. Em seguida, o conteúdo HTML da página é extraído e processado, sendo então analisado e armazenado na variável `pagina` utilizando a biblioteca `BeautifulSoup`.\n",
    "\n",
    "Será também criada uma função `is_crawl_allowed`. Ela é necessária para verificar se o web crawling é permitido em uma página específica de acordo com as regras definidas no arquivo robots.txt do site. O arquivo robots.txt contém diretrizes para web crawlers, especificando quais páginas ou seções de um site podem ser acessadas por bots e quais não podem. Isso ajuda a proteger servidores contra acessos não autorizados ou excessivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61a7dd84",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-14T12:07:06.496506Z",
     "iopub.status.busy": "2024-08-14T12:07:06.495895Z",
     "iopub.status.idle": "2024-08-14T12:07:08.244983Z",
     "shell.execute_reply": "2024-08-14T12:07:08.243559Z"
    },
    "papermill": {
     "duration": 1.762919,
     "end_time": "2024-08-14T12:07:08.249963",
     "exception": false,
     "start_time": "2024-08-14T12:07:06.487044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função para verificar se o crawling é permitido\n",
    "def is_crawl_allowed(url: str, user_agent: str) -> bool:\n",
    "    # Define a URL do arquivo robots.txt com base no domínio da URL fornecida\n",
    "    robots_url = 'https://github.com/robots.txt'\n",
    "    \n",
    "    # Inicializa o parser de robots.txt\n",
    "    rp = RobotFileParser()\n",
    "    \n",
    "    # Lê o arquivo robots.txt do GitHub\n",
    "    rp.set_url(robots_url)\n",
    "    rp.read()\n",
    "    \n",
    "    # Verifica se o crawling da URL fornecida é permitido para o User-Agent especificado\n",
    "    return rp.can_fetch(user_agent, url)\n",
    "\n",
    "# Função para realizar o web crawl na página de Trendings do GitHub\n",
    "def crawl_website(url: str, headers: dict) -> str:\n",
    "    # Verifica se o crawling é permitido antes de tentar acessar a página\n",
    "    if is_crawl_allowed(url, headers['User-Agent']):\n",
    "        try:\n",
    "            # Faz uma requisição GET à URL fornecida, usando os cabeçalhos especificados\n",
    "            resposta = requests.get(url, headers=headers)\n",
    "            \n",
    "            # Verifica se houve algum erro na requisição, como um status code diferente de 200\n",
    "            resposta.raise_for_status()\n",
    "        except HTTPError as exc:\n",
    "            # Se ocorrer um erro HTTP, imprime a exceção\n",
    "            print(exc)\n",
    "        else:\n",
    "            # Se não houver erros, retorna o conteúdo da resposta (o HTML da página)\n",
    "            return resposta.text\n",
    "    else:\n",
    "        print(\"Crawling não permitido para esta página de acordo com o arquivo robots.txt.\")\n",
    "        return None\n",
    "\n",
    "# URL da página de Trendings do GitHub\n",
    "URL = 'https://github.com/trending'\n",
    "\n",
    "# Cabeçalhos HTTP para imitar uma requisição feita por um navegador comum, o que ajuda a evitar bloqueios\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "    'Accept-Language': 'pt-BR,pt;q=0.9'  # Especifica a preferência de idioma para a resposta\n",
    "}\n",
    "\n",
    "# Chama a função para realizar o crawl na URL especificada e armazena o conteúdo da página\n",
    "conteudo = crawl_website(url=URL, headers=HEADERS)\n",
    "\n",
    "# Se o conteúdo foi obtido com sucesso, realiza o parse com BeautifulSoup\n",
    "if conteudo:\n",
    "    # Utiliza BeautifulSoup para fazer o parse (análise) do conteúdo HTML obtido\n",
    "    pagina = BeautifulSoup(conteudo, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db83f85",
   "metadata": {
    "papermill": {
     "duration": 0.009442,
     "end_time": "2024-08-14T12:07:08.273487",
     "exception": false,
     "start_time": "2024-08-14T12:07:08.264045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4\\. Manipulação\n",
    "\n",
    "Nesta etapa, organizaremos os dados extraídos da página. Como os conteúdos estão distribuídos em diferentes elementos **HTML**, utilizaremos classes **CSS** específicas para localizar essas informações. Em seguida, iteraremos sobre as listas em Python para extrair e estruturar os dados de forma coerente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f6887",
   "metadata": {
    "papermill": {
     "duration": 0.008844,
     "end_time": "2024-08-14T12:07:08.291423",
     "exception": false,
     "start_time": "2024-08-14T12:07:08.282579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "No código abaixo, os títulos e autores dos repositórios são capturados a partir de elementos `<h2>` da página. Após a extração, essas informações são armazenadas nas listas correspondentes para serem manipuladas posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d733d63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T12:07:08.311841Z",
     "iopub.status.busy": "2024-08-14T12:07:08.311375Z",
     "iopub.status.idle": "2024-08-14T12:07:08.388840Z",
     "shell.execute_reply": "2024-08-14T12:07:08.387421Z"
    },
    "papermill": {
     "duration": 0.092472,
     "end_time": "2024-08-14T12:07:08.393043",
     "exception": false,
     "start_time": "2024-08-14T12:07:08.300571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autores: \n",
      " ['versotile-org', 'grafana', 'ostris', 'DroidKaigi', 'hacksider', 'TheAlgorithms', 'geekan', 'timlrx', 'erincatto', 'twbs', 'electric-sql', 'Stirling-Tools', 'SimplifyJobs', 'xorbitsai', 'axios', 'snipe']\n",
      "\n",
      "Títulos: \n",
      " ['verso', 'grafana', 'ai-toolkit', 'conference-app-2024', 'Deep-Live-Cam', 'Go', 'MetaGPT', 'tailwind-nextjs-starter-blog', 'box2d', 'bootstrap', 'pglite', 'Stirling-PDF', 'Summer2025-Internships', 'inference', 'axios', 'snipe-it']\n"
     ]
    }
   ],
   "source": [
    "# Inicializa as listas para armazenar os títulos dos repositórios e os nomes dos autores\n",
    "titulos = []\n",
    "autores = []\n",
    "\n",
    "# Encontra todos os elementos <h2> que contêm os projetos na página, utilizando a classe CSS específica\n",
    "projects = pagina.find_all('h2', class_='h3 lh-condensed')\n",
    "\n",
    "# Itera sobre cada elemento <h2> encontrado\n",
    "for h2 in projects:\n",
    "    # Dentro de cada <h2>, encontra todos os elementos <a> que contêm os links para os repositórios, usando a classe CSS específica\n",
    "    linhas = h2.find_all('a', class_='Link')\n",
    "    \n",
    "    # Itera sobre cada link <a> encontrado\n",
    "    for linha in linhas:\n",
    "        # Adiciona o título do repositório à lista 'titulos'\n",
    "        # O texto é limpo removendo quebras de linha, separando o autor e o repositório por '/', e retirando espaços em branco\n",
    "        titulos.append(linha.get_text().replace('\\n', \"\").split(\"/\")[1].strip(\" \"))\n",
    "        \n",
    "        # Adiciona o nome do autor à lista 'autores'\n",
    "        # O processo é semelhante ao acima, mas captura a primeira parte do texto que contém o nome do autor\n",
    "        autores.append(linha.get_text().replace('\\n', \"\").split(\"/\")[0].strip(\" \"))\n",
    "\n",
    "# Encontra todos os elementos <span> que contêm os nomes dos autores, usando a classe CSS específica\n",
    "projectsAuthor = pagina.find_all('span', class_='text-normal')\n",
    "\n",
    "# Exibe a lista de autores capturados\n",
    "print(\"Autores: \\n\", autores)\n",
    "\n",
    "# Exibe a lista de títulos capturados\n",
    "print(\"\\nTítulos: \\n\", titulos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fae6f8c",
   "metadata": {
    "papermill": {
     "duration": 0.01086,
     "end_time": "2024-08-14T12:07:08.414300",
     "exception": false,
     "start_time": "2024-08-14T12:07:08.403440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Agora, iremos coletar as informações de `stars` e `forks` dos repositórios. Esses dados estão localizados em elementos de link (`<a>`) dentro de elementos de artigo (`<article>`). Como ambos os valores estão no mesmo tipo de elemento, utilizamos uma verificação para identificar os caracteres numéricos. No entanto, isso inicialmente fez com que `stars` e `forks` fossem armazenados na mesma lista. Para resolver esse problema, intercalamos a iteração, alternando entre as listas. Usamos o índice da iteração para determinar se o valor deve ser adicionado à lista `forks` (quando o índice é par) ou à lista `stars` (quando o índice é ímpar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6e1682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T12:07:08.433980Z",
     "iopub.status.busy": "2024-08-14T12:07:08.433578Z",
     "iopub.status.idle": "2024-08-14T12:07:08.470550Z",
     "shell.execute_reply": "2024-08-14T12:07:08.469098Z"
    },
    "papermill": {
     "duration": 0.04985,
     "end_time": "2024-08-14T12:07:08.474103",
     "exception": false,
     "start_time": "2024-08-14T12:07:08.424253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stars: \n",
      " [2882, 61978, 655, 266, 21167, 15329, 42648, 8122, 7725, 168666, 6164, 37777, 32735, 4167, 104886, 10580]\n",
      "\n",
      "Forks: \n",
      " [121, 11803, 56, 91, 2775, 2530, 5081, 1925, 1488, 78604, 103, 2816, 2652, 339, 10824, 3097]\n"
     ]
    }
   ],
   "source": [
    "# Encontra todos os elementos <article> que contêm as informações dos projetos,\n",
    "projectsInfos = pagina.find_all('article', class_='Box-row')\n",
    "\n",
    "# Inicializa as listas para armazenar o número de estrelas e forks dos repositórios\n",
    "stars = []\n",
    "forks = []\n",
    "\n",
    "# Itera sobre cada <article> encontrado, que contém as informações dos projetos\n",
    "for article in projectsInfos:\n",
    "    # Dentro de cada <article>, encontra todos os elementos <a> que contêm links, utilizando a classe CSS específica\n",
    "    linhas = article.find_all('a', class_='Link')\n",
    "    \n",
    "    # Itera sobre cada link <a> encontrado\n",
    "    for i, linha in enumerate(linhas):\n",
    "        # Extrai o texto do link, remove espaços em branco e vírgulas\n",
    "        texto = linha.get_text().strip().replace(',', '') \n",
    "        \n",
    "        # Verifica se o texto não está vazio e se começa com um dígito (número)\n",
    "        if texto and texto[0].isdigit():\n",
    "            # Converte o texto para um número inteiro\n",
    "            valor = int(texto)\n",
    "            \n",
    "            # Alterna entre adicionar o valor na lista 'forks' ou 'stars' com base no índice (par ou ímpar)\n",
    "            (forks if i % 2 == 0 else stars).append(valor)\n",
    "\n",
    "# Exibe a lista de estrelas capturadas\n",
    "print(\"Stars: \\n\", stars)\n",
    "\n",
    "# Exibe a lista de forks capturados\n",
    "print(\"\\nForks: \\n\", forks)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e827e1f",
   "metadata": {
    "papermill": {
     "duration": 0.007025,
     "end_time": "2024-08-14T12:07:08.490818",
     "exception": false,
     "start_time": "2024-08-14T12:07:08.483793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Nesta etapa, vamos capturar as estrelas recebidas hoje (`stars today`) de cada repositório. Esses dados serão extraídos de elementos com a tag `<span>` da classe `d-inline-block float-sm-right`. Após a extração, as informações serão armazenadas na lista `stars_today`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6784e995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T12:07:08.507167Z",
     "iopub.status.busy": "2024-08-14T12:07:08.506757Z",
     "iopub.status.idle": "2024-08-14T12:07:08.522510Z",
     "shell.execute_reply": "2024-08-14T12:07:08.520827Z"
    },
    "papermill": {
     "duration": 0.027045,
     "end_time": "2024-08-14T12:07:08.525245",
     "exception": false,
     "start_time": "2024-08-14T12:07:08.498200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stars Today: \n",
      " [726, 20, 107, 40, 3539, 201, 176, 226, 88, 46, 551, 495, 47, 15, 27, 8]\n"
     ]
    }
   ],
   "source": [
    "# Inicializa a lista para armazenar as estrelas recebidas hoje para cada repositório\n",
    "stars_today = []\n",
    "\n",
    "# Itera sobre cada artigo (repositório) encontrado anteriormente em 'projectsInfos'\n",
    "for article in projectsInfos:\n",
    "    # Dentro de cada <article>, encontra todos os elementos <span> que contêm as estrelas de hoje,\n",
    "    # utilizando a classe CSS específica\n",
    "    linhas = article.find_all('span', class_='d-inline-block float-sm-right')\n",
    "    \n",
    "    # Itera sobre cada elemento <span> encontrado\n",
    "    for linha in linhas:\n",
    "        # Extrai o texto do elemento, removendo quebras de linha, espaços em branco e vírgulas\n",
    "        # Em seguida, separa o texto pelo espaço em branco e pega o primeiro valor (número de estrelas)\n",
    "        linha = linha.get_text().replace('\\n', \"\").strip().replace(\",\", \"\").split(\" \")[0]\n",
    "        \n",
    "        # Converte o texto do número de estrelas para um inteiro\n",
    "        valor = int(linha)\n",
    "        \n",
    "        # Adiciona o valor (número de estrelas de hoje) à lista 'stars_today'\n",
    "        stars_today.append(valor)\n",
    "\n",
    "# Exibe a lista de estrelas recebidas hoje capturadas\n",
    "print(\"Stars Today: \\n\", stars_today)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e761cf6",
   "metadata": {
    "papermill": {
     "duration": 0.007044,
     "end_time": "2024-08-14T12:07:08.539792",
     "exception": false,
     "start_time": "2024-08-14T12:07:08.532748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Continuando a coleta dos dados, agora capturaremos as linguagens de programação utilizadas nos projetos. Essas informações estão presentes em elementos `<span>` com a classe `d-inline-block ml-0 mr-3`, localizados dentro dos elementos `<article>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c5b5ccb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T12:07:08.557083Z",
     "iopub.status.busy": "2024-08-14T12:07:08.556657Z",
     "iopub.status.idle": "2024-08-14T12:07:08.571158Z",
     "shell.execute_reply": "2024-08-14T12:07:08.569676Z"
    },
    "papermill": {
     "duration": 0.02688,
     "end_time": "2024-08-14T12:07:08.574051",
     "exception": false,
     "start_time": "2024-08-14T12:07:08.547171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rust', 'TypeScript', 'Python', 'Kotlin', 'Python', 'Go', 'Python', 'TypeScript', 'C', 'JavaScript', 'PLpgSQL', 'Java', 'Python', 'JavaScript', 'PHP']\n"
     ]
    }
   ],
   "source": [
    "# Inicializa a lista para armazenar as linguagens de programação utilizadas nos projetos\n",
    "languages = []\n",
    "\n",
    "# Itera sobre cada artigo (repositório) encontrado anteriormente em 'projectsInfos'\n",
    "for article in projectsInfos:\n",
    "    # Dentro de cada <article>, encontra todos os elementos <span> que contêm as linguagens,\n",
    "    # utilizando a classe CSS específica\n",
    "    linhas = article.find_all('span', class_='d-inline-block ml-0 mr-3')\n",
    "    \n",
    "    # Itera sobre cada elemento <span> encontrado\n",
    "    for linha in linhas:\n",
    "        # Extrai o texto do elemento, removendo quebras de linha e espaços em branco\n",
    "        linha = linha.get_text().replace('\\n', \"\").strip()\n",
    "        \n",
    "        # Adiciona a linguagem de programação à lista 'languages'\n",
    "        languages.append(linha)\n",
    "\n",
    "# Exibe a lista de linguagens de programação capturadas\n",
    "print(languages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e4ea5",
   "metadata": {
    "papermill": {
     "duration": 0.0072,
     "end_time": "2024-08-14T12:07:08.589974",
     "exception": false,
     "start_time": "2024-08-14T12:07:08.582774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Após concluir a coleta dos dados, agora vamos combinar as listas coletadas. Essas listas serão integradas em uma única variável e, em seguida, organizadas em um DataFrame do Pandas para facilitar a visualização da tabela. Além disso, será criado um índice chamado `rank` no DataFrame e ordenado pela quantidade de `stars` de forma decrescente e exibirá a posição de cada repositório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e60b0672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T12:07:08.608078Z",
     "iopub.status.busy": "2024-08-14T12:07:08.607662Z",
     "iopub.status.idle": "2024-08-14T12:07:08.658645Z",
     "shell.execute_reply": "2024-08-14T12:07:08.657238Z"
    },
    "papermill": {
     "duration": 0.063046,
     "end_time": "2024-08-14T12:07:08.661556",
     "exception": false,
     "start_time": "2024-08-14T12:07:08.598510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>author</th>\n",
       "      <th>project</th>\n",
       "      <th>language</th>\n",
       "      <th>stars</th>\n",
       "      <th>stars_today</th>\n",
       "      <th>forks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>twbs</td>\n",
       "      <td>bootstrap</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>168666</td>\n",
       "      <td>46</td>\n",
       "      <td>78604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>axios</td>\n",
       "      <td>axios</td>\n",
       "      <td>PHP</td>\n",
       "      <td>104886</td>\n",
       "      <td>27</td>\n",
       "      <td>10824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>grafana</td>\n",
       "      <td>grafana</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>61978</td>\n",
       "      <td>20</td>\n",
       "      <td>11803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>geekan</td>\n",
       "      <td>MetaGPT</td>\n",
       "      <td>Python</td>\n",
       "      <td>42648</td>\n",
       "      <td>176</td>\n",
       "      <td>5081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>Stirling-Tools</td>\n",
       "      <td>Stirling-PDF</td>\n",
       "      <td>Java</td>\n",
       "      <td>37777</td>\n",
       "      <td>495</td>\n",
       "      <td>2816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>SimplifyJobs</td>\n",
       "      <td>Summer2025-Internships</td>\n",
       "      <td>Python</td>\n",
       "      <td>32735</td>\n",
       "      <td>47</td>\n",
       "      <td>2652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>hacksider</td>\n",
       "      <td>Deep-Live-Cam</td>\n",
       "      <td>Python</td>\n",
       "      <td>21167</td>\n",
       "      <td>3539</td>\n",
       "      <td>2775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>TheAlgorithms</td>\n",
       "      <td>Go</td>\n",
       "      <td>Go</td>\n",
       "      <td>15329</td>\n",
       "      <td>201</td>\n",
       "      <td>2530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>timlrx</td>\n",
       "      <td>tailwind-nextjs-starter-blog</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>8122</td>\n",
       "      <td>226</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>erincatto</td>\n",
       "      <td>box2d</td>\n",
       "      <td>C</td>\n",
       "      <td>7725</td>\n",
       "      <td>88</td>\n",
       "      <td>1488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>electric-sql</td>\n",
       "      <td>pglite</td>\n",
       "      <td>PLpgSQL</td>\n",
       "      <td>6164</td>\n",
       "      <td>551</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>xorbitsai</td>\n",
       "      <td>inference</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>4167</td>\n",
       "      <td>15</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>versotile-org</td>\n",
       "      <td>verso</td>\n",
       "      <td>Rust</td>\n",
       "      <td>2882</td>\n",
       "      <td>726</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>ostris</td>\n",
       "      <td>ai-toolkit</td>\n",
       "      <td>Python</td>\n",
       "      <td>655</td>\n",
       "      <td>107</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>DroidKaigi</td>\n",
       "      <td>conference-app-2024</td>\n",
       "      <td>Kotlin</td>\n",
       "      <td>266</td>\n",
       "      <td>40</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank          author                       project    language   stars  \\\n",
       "9      1            twbs                     bootstrap  JavaScript  168666   \n",
       "14     2           axios                         axios         PHP  104886   \n",
       "1      3         grafana                       grafana  TypeScript   61978   \n",
       "6      4          geekan                       MetaGPT      Python   42648   \n",
       "11     5  Stirling-Tools                  Stirling-PDF        Java   37777   \n",
       "12     6    SimplifyJobs        Summer2025-Internships      Python   32735   \n",
       "4      7       hacksider                 Deep-Live-Cam      Python   21167   \n",
       "5      8   TheAlgorithms                            Go          Go   15329   \n",
       "7      9          timlrx  tailwind-nextjs-starter-blog  TypeScript    8122   \n",
       "8     10       erincatto                         box2d           C    7725   \n",
       "10    11    electric-sql                        pglite     PLpgSQL    6164   \n",
       "13    12       xorbitsai                     inference  JavaScript    4167   \n",
       "0     13   versotile-org                         verso        Rust    2882   \n",
       "2     14          ostris                    ai-toolkit      Python     655   \n",
       "3     15      DroidKaigi           conference-app-2024      Kotlin     266   \n",
       "\n",
       "    stars_today  forks  \n",
       "9            46  78604  \n",
       "14           27  10824  \n",
       "1            20  11803  \n",
       "6           176   5081  \n",
       "11          495   2816  \n",
       "12           47   2652  \n",
       "4          3539   2775  \n",
       "5           201   2530  \n",
       "7           226   1925  \n",
       "8            88   1488  \n",
       "10          551    103  \n",
       "13           15    339  \n",
       "0           726    121  \n",
       "2           107     56  \n",
       "3            40     91  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combina as listas de autores, títulos, linguagens, estrelas, estrelas de hoje e forks em uma única lista de tuplas.\n",
    "# Cada tupla contém os dados de um repositório específico.\n",
    "conteudo_extraido = list(zip(autores, titulos, languages, stars, stars_today, forks))\n",
    "\n",
    "# Cria um DataFrame do Pandas a partir dos dados combinados, com as colunas especificadas.\n",
    "# As colunas são: 'author', 'project', 'language', 'stars', 'stars_today', e 'forks'.\n",
    "projects_df = pd.DataFrame(conteudo_extraido, columns=['author','project','language', 'stars', 'stars_today','forks'])\n",
    "\n",
    "# Ordena o DataFrame 'projects_df' pela coluna 'stars' em ordem decrescente (do maior para o menor)\n",
    "projects_df = projects_df.sort_values(by='stars', ascending=False)\n",
    "\n",
    "# Adiciona uma nova coluna 'rank' ao DataFrame, que contém a posição de cada repositório.\n",
    "# O rank é gerado como uma sequência numérica de 1 a até o tamanho da lista.\n",
    "projects_df['rank'] = range(1, len(projects_df)+1)\n",
    "\n",
    "# Reorganiza as colunas do DataFrame para exibir o rank na primeira posição.\n",
    "# A nova ordem das colunas é: 'rank', 'author', 'project', 'language', 'stars', 'stars_today', 'forks'.\n",
    "projects_df = projects_df[['rank','author','project','language', 'stars', 'stars_today','forks']]\n",
    "\n",
    "# Exibe o DataFrame resultante, mostrando os dados organizados de forma tabular.\n",
    "projects_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ea626",
   "metadata": {
    "papermill": {
     "duration": 0.007787,
     "end_time": "2024-08-14T12:07:08.677555",
     "exception": false,
     "start_time": "2024-08-14T12:07:08.669768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5\\. Salvamento\n",
    "\n",
    "Por fim, todos os dados tratados e organizados serão salvos em um arquivo CSV, utilizando ponto e vírgula (;) como delimitador entre os campos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c9c696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T12:07:08.697772Z",
     "iopub.status.busy": "2024-08-14T12:07:08.695821Z",
     "iopub.status.idle": "2024-08-14T12:07:08.707731Z",
     "shell.execute_reply": "2024-08-14T12:07:08.706463Z"
    },
    "papermill": {
     "duration": 0.02543,
     "end_time": "2024-08-14T12:07:08.711056",
     "exception": false,
     "start_time": "2024-08-14T12:07:08.685626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Salvar em arquivo csv\n",
    "projects_df.to_csv('github.csv', sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.871698,
   "end_time": "2024-08-14T12:07:09.241954",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-14T12:07:01.370256",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
