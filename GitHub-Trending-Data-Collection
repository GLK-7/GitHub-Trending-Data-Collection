{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **GitHub-Trending Data Collection**\n\n---\n\nAutor: **Gabriel Lino**\n\n<div style=\"display: flex; gap: 10px;\"> \n  <a href=\"mailto:gabriel.godoitb@gmail.com\"><img src=\"https://img.shields.io/badge/-Gmail-%23333?style=for-the-badge&logo=gmail&logoColor=white\" target=\"_blank\"></a>\n  <a href=\"https://www.linkedin.com/in/glgodoi\" target=\"_blank\"><img src=\"https://img.shields.io/badge/-LinkedIn-%230077B5?style=for-the-badge&logo=linkedin&logoColor=white\" target=\"_blank\"></a>  \n  <a href=\"https://github.com/GLK-7\" target=\"_blank\"><img src=\"https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white\" target=\"_blank\"></a>  \n</div>\n\n---","metadata":{}},{"cell_type":"markdown","source":"# **Tópicos**\n\n<ol type=\"1\">\n    <li>Contexto;</li>\n    <li>Pacotes e Bibliotecas;</li>\n    <li>Extração;</li>\n    <li>Manipulação.</li>\n    <li>Salvamento.</li>\n</ol>","metadata":{}},{"cell_type":"markdown","source":"## 1. Contexto\n\n<div style=\"display: flex; align-items: center; flex-direction: column; margin-bottom: 20px;\">\n    <div style=\"display: inline-block; background-color: white; border-radius: 10px; padding: 10px;\">\n        <img width=\"180em\" height=\"auto\" src=\"https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/github/github-original-wordmark.svg\" alt=\"GitHub Logo\"/>\n    </div>\n    <p style=\"margin-left: 10px;\">\n        O <a href=\"https://github.com/\">GitHub</a> é uma plataforma de hospedagem de código-fonte e desenvolvimento colaborativo que permite que desenvolvedores ao redor do mundo compartilhem e colaborem em projetos. Ele se destaca como um dos maiores repositórios de código aberto, onde projetos populares são frequentemente destacados na seção \"Trending\", que mostra os repositórios mais relevantes e ativos em determinado período.\n    </p>\n</div>\n<p>\nNeste projeto, realizaremos a coleta de dados da seção \"Trending\" do GitHub, utilizando técnicas de web crawling com Python. O objetivo é capturar informações sobre os repositórios em destaque, como nome, descrição, linguagem de programação utilizada, número de estrelas, entre outros. Os dados coletados serão organizados e salvos em um arquivo CSV, permitindo análises futuras sobre as tendências e popularidade de projetos no GitHub.\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"## 2\\. Pacotes e Bibliotecas\n\nAbaixo será feita a importação dos pacotes e bibliotecas utilizados nesse projeto.","metadata":{}},{"cell_type":"code","source":"# Importar Pacotes e Bibliotecas do Python\nimport requests\nimport pandas as pd\nimport csv\nfrom bs4 import BeautifulSoup\nfrom requests.exceptions import HTTPError","metadata":{"execution":{"iopub.status.busy":"2024-08-13T22:50:24.741151Z","iopub.execute_input":"2024-08-13T22:50:24.741532Z","iopub.status.idle":"2024-08-13T22:50:25.527163Z","shell.execute_reply.started":"2024-08-13T22:50:24.741503Z","shell.execute_reply":"2024-08-13T22:50:25.526119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3\\. Extração","metadata":{}},{"cell_type":"markdown","source":"#### Crawling\n\nWeb crawling é uma técnica utilizada para coletar dados de sites automaticamente. Com Python, essa técnica envolve o uso de bibliotecas como `BeautifulSoup` e `requests` para acessar páginas da web, extrair informações relevantes e estruturar esses dados de forma organizada","metadata":{}},{"cell_type":"markdown","source":"No código a seguir, primeiramente, é realizada a conexão com a página de \"Trending\" do GitHub. Em seguida, o conteúdo HTML da página é extraído e processado, sendo então analisado e armazenado na variável `pagina` utilizando a biblioteca `BeautifulSoup`.","metadata":{}},{"cell_type":"code","source":"# Função para realizar o web crawl na página de Trendings do GitHub\ndef crawl_website(url: str, headers: dict) -> str:\n    try:\n        # Faz uma requisição GET à URL fornecida, usando os cabeçalhos especificados\n        resposta = requests.get(url, headers=headers)\n        \n        # Verifica se houve algum erro na requisição, como um status code diferente de 200\n        resposta.raise_for_status()\n    except HTTPError as exc:\n        # Se ocorrer um erro HTTP, imprime a exceção\n        print(exc)\n    else:\n        # Se não houver erros, retorna o conteúdo da resposta (o HTML da página)\n        return resposta.text\n\n# URL da página de Trendings do GitHub\nURL = 'https://github.com/trending'\n\n# Cabeçalhos HTTP para imitar uma requisição feita por um navegador comum, o que ajuda a evitar bloqueios\nHEADERS = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n    'Accept-Language': 'pt-BR,pt;q=0.9'  # Especifica a preferência de idioma para a resposta\n}\n\n# Chama a função para realizar o crawl na URL especificada e armazena o conteúdo da página\nconteudo = crawl_website(url=URL, headers=HEADERS)\n\n# Utiliza BeautifulSoup para fazer o parse (análise) do conteúdo HTML obtido\npagina = BeautifulSoup(conteudo, 'html.parser')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-13T22:50:35.170139Z","iopub.execute_input":"2024-08-13T22:50:35.170568Z","iopub.status.idle":"2024-08-13T22:50:36.564008Z","shell.execute_reply.started":"2024-08-13T22:50:35.170529Z","shell.execute_reply":"2024-08-13T22:50:36.562680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4\\. Manipulação\n\nNesta etapa, organizaremos os dados extraídos da página. Como os conteúdos estão distribuídos em diferentes elementos **HTML**, utilizaremos classes **CSS** específicas para localizar essas informações. Em seguida, iteraremos sobre as listas em Python para extrair e estruturar os dados de forma coerente.","metadata":{}},{"cell_type":"markdown","source":"No código abaixo, os títulos e autores dos repositórios são capturados a partir de elementos `<h2>` da página. Após a extração, essas informações são armazenadas nas listas correspondentes para serem manipuladas posteriormente.","metadata":{}},{"cell_type":"code","source":"# Inicializa as listas para armazenar os títulos dos repositórios e os nomes dos autores\ntitulos = []\nautores = []\n\n# Encontra todos os elementos <h2> que contêm os projetos na página, utilizando a classe CSS específica\nprojects = pagina.find_all('h2', class_='h3 lh-condensed')\n\n# Itera sobre cada elemento <h2> encontrado\nfor h2 in projects:\n    # Dentro de cada <h2>, encontra todos os elementos <a> que contêm os links para os repositórios, usando a classe CSS específica\n    linhas = h2.find_all('a', class_='Link')\n    \n    # Itera sobre cada link <a> encontrado\n    for linha in linhas:\n        # Adiciona o título do repositório à lista 'titulos'\n        # O texto é limpo removendo quebras de linha, separando o autor e o repositório por '/', e retirando espaços em branco\n        titulos.append(linha.get_text().replace('\\n', \"\").split(\"/\")[1].strip(\" \"))\n        \n        # Adiciona o nome do autor à lista 'autores'\n        # O processo é semelhante ao acima, mas captura a primeira parte do texto que contém o nome do autor\n        autores.append(linha.get_text().replace('\\n', \"\").split(\"/\")[0].strip(\" \"))\n\n# Encontra todos os elementos <span> que contêm os nomes dos autores, usando a classe CSS específica\nprojectsAuthor = pagina.find_all('span', class_='text-normal')\n\n# Exibe a lista de autores capturados\nprint(\"Autores: \\n\", autores)\n\n# Exibe a lista de títulos capturados\nprint(\"\\nTítulos: \\n\", titulos)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T22:50:39.067235Z","iopub.execute_input":"2024-08-13T22:50:39.067663Z","iopub.status.idle":"2024-08-13T22:50:39.128259Z","shell.execute_reply.started":"2024-08-13T22:50:39.067627Z","shell.execute_reply":"2024-08-13T22:50:39.126996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Agora, iremos coletar as informações de `stars` e `forks` dos repositórios. Esses dados estão localizados em elementos de link (`<a>`) dentro de elementos de artigo (`<article>`). Como ambos os valores estão no mesmo tipo de elemento, utilizamos uma verificação para identificar os caracteres numéricos. No entanto, isso inicialmente fez com que `stars` e `forks` fossem armazenados na mesma lista. Para resolver esse problema, intercalamos a iteração, alternando entre as listas. Usamos o índice da iteração para determinar se o valor deve ser adicionado à lista `forks` (quando o índice é par) ou à lista `stars` (quando o índice é ímpar).","metadata":{}},{"cell_type":"code","source":"# Encontra todos os elementos <article> que contêm as informações dos projetos,\nprojectsInfos = pagina.find_all('article', class_='Box-row')\n\n# Inicializa as listas para armazenar o número de estrelas e forks dos repositórios\nstars = []\nforks = []\n\n# Itera sobre cada <article> encontrado, que contém as informações dos projetos\nfor article in projectsInfos:\n    # Dentro de cada <article>, encontra todos os elementos <a> que contêm links, utilizando a classe CSS específica\n    linhas = article.find_all('a', class_='Link')\n    \n    # Itera sobre cada link <a> encontrado\n    for i, linha in enumerate(linhas):\n        # Extrai o texto do link, remove espaços em branco e vírgulas\n        texto = linha.get_text().strip().replace(',', '') \n        \n        # Verifica se o texto não está vazio e se começa com um dígito (número)\n        if texto and texto[0].isdigit():\n            # Converte o texto para um número inteiro\n            valor = int(texto)\n            \n            # Alterna entre adicionar o valor na lista 'forks' ou 'stars' com base no índice (par ou ímpar)\n            (forks if i % 2 == 0 else stars).append(valor)\n\n# Exibe a lista de estrelas capturadas\nprint(\"Stars: \\n\", stars)\n\n# Exibe a lista de forks capturados\nprint(\"\\nForks: \\n\", forks)\n\n ","metadata":{"execution":{"iopub.status.busy":"2024-08-13T22:50:44.730607Z","iopub.execute_input":"2024-08-13T22:50:44.731036Z","iopub.status.idle":"2024-08-13T22:50:44.767741Z","shell.execute_reply.started":"2024-08-13T22:50:44.731001Z","shell.execute_reply":"2024-08-13T22:50:44.766509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nesta etapa, vamos capturar as estrelas recebidas hoje (`stars today`) de cada repositório. Esses dados serão extraídos de elementos com a tag `<span>` da classe `d-inline-block float-sm-right`. Após a extração, as informações serão armazenadas na lista `stars_today`.","metadata":{}},{"cell_type":"code","source":"# Inicializa a lista para armazenar as estrelas recebidas hoje para cada repositório\nstars_today = []\n\n# Itera sobre cada artigo (repositório) encontrado anteriormente em 'projectsInfos'\nfor article in projectsInfos:\n    # Dentro de cada <article>, encontra todos os elementos <span> que contêm as estrelas de hoje,\n    # utilizando a classe CSS específica\n    linhas = article.find_all('span', class_='d-inline-block float-sm-right')\n    \n    # Itera sobre cada elemento <span> encontrado\n    for linha in linhas:\n        # Extrai o texto do elemento, removendo quebras de linha, espaços em branco e vírgulas\n        # Em seguida, separa o texto pelo espaço em branco e pega o primeiro valor (número de estrelas)\n        linha = linha.get_text().replace('\\n', \"\").strip().replace(\",\", \"\").split(\" \")[0]\n        \n        # Converte o texto do número de estrelas para um inteiro\n        valor = int(linha)\n        \n        # Adiciona o valor (número de estrelas de hoje) à lista 'stars_today'\n        stars_today.append(valor)\n\n# Exibe a lista de estrelas recebidas hoje capturadas\nprint(\"Stars Today: \\n\", stars_today)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T22:51:46.036237Z","iopub.execute_input":"2024-08-13T22:51:46.036715Z","iopub.status.idle":"2024-08-13T22:51:46.053305Z","shell.execute_reply.started":"2024-08-13T22:51:46.036680Z","shell.execute_reply":"2024-08-13T22:51:46.051965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Continuando a coleta dos dados, agora capturaremos as linguagens de programação utilizadas nos projetos. Essas informações estão presentes em elementos `<span>` com a classe `d-inline-block ml-0 mr-3`, localizados dentro dos elementos `<article>`.","metadata":{}},{"cell_type":"code","source":"# Inicializa a lista para armazenar as linguagens de programação utilizadas nos projetos\nlanguages = []\n\n# Itera sobre cada artigo (repositório) encontrado anteriormente em 'projectsInfos'\nfor article in projectsInfos:\n    # Dentro de cada <article>, encontra todos os elementos <span> que contêm as linguagens,\n    # utilizando a classe CSS específica\n    linhas = article.find_all('span', class_='d-inline-block ml-0 mr-3')\n    \n    # Itera sobre cada elemento <span> encontrado\n    for linha in linhas:\n        # Extrai o texto do elemento, removendo quebras de linha e espaços em branco\n        linha = linha.get_text().replace('\\n', \"\").strip()\n        \n        # Adiciona a linguagem de programação à lista 'languages'\n        languages.append(linha)\n\n# Exibe a lista de linguagens de programação capturadas\nprint(languages)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T22:51:48.337224Z","iopub.execute_input":"2024-08-13T22:51:48.337672Z","iopub.status.idle":"2024-08-13T22:51:48.354857Z","shell.execute_reply.started":"2024-08-13T22:51:48.337636Z","shell.execute_reply":"2024-08-13T22:51:48.353392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Após concluir a coleta dos dados, agora vamos combinar as listas coletadas. Essas listas serão integradas em uma única variável e, em seguida, organizadas em um DataFrame do Pandas para facilitar a visualização da tabela. Além disso, será criado um índice chamado `rank` no DataFrame e ordenado pela quantidade de `stars` de forma decrescente e exibirá a posição de cada repositório.","metadata":{}},{"cell_type":"code","source":"# Combina as listas de autores, títulos, linguagens, estrelas, estrelas de hoje e forks em uma única lista de tuplas.\n# Cada tupla contém os dados de um repositório específico.\nconteudo_extraido = list(zip(autores, titulos, languages, stars, stars_today, forks))\n\n# Cria um DataFrame do Pandas a partir dos dados combinados, com as colunas especificadas.\n# As colunas são: 'author', 'project', 'language', 'stars', 'stars_today', e 'forks'.\nprojects_df = pd.DataFrame(conteudo_extraido, columns=['author','project','language', 'stars', 'stars_today','forks'])\n\n# Ordena o DataFrame 'projects_df' pela coluna 'stars' em ordem decrescente (do maior para o menor)\nprojects_df = projects_df.sort_values(by='stars', ascending=False)\n\n# Adiciona uma nova coluna 'rank' ao DataFrame, que contém a posição de cada repositório.\n# O rank é gerado como uma sequência numérica de 1 a 10, correspondendo à posição no trending.\nprojects_df['rank'] = range(1, 21)\n\n# Reorganiza as colunas do DataFrame para exibir o rank na primeira posição.\n# A nova ordem das colunas é: 'rank', 'author', 'project', 'language', 'stars', 'stars_today', 'forks'.\nprojects_df = projects_df[['rank','author','project','language', 'stars', 'stars_today','forks']]\n\n# Exibe o DataFrame resultante, mostrando os dados organizados de forma tabular.\nprojects_df\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T22:52:09.008923Z","iopub.execute_input":"2024-08-13T22:52:09.009560Z","iopub.status.idle":"2024-08-13T22:52:09.046243Z","shell.execute_reply.started":"2024-08-13T22:52:09.009525Z","shell.execute_reply":"2024-08-13T22:52:09.045091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5\\. Salvamento\n\nPor fim, todos os dados tratados e organizados serão salvos em um arquivo CSV, utilizando ponto e vírgula (;) como delimitador entre os campos.","metadata":{}},{"cell_type":"code","source":"# Salvar em arquivo csv\nprojects_df.to_csv('github.csv', sep=';', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T22:54:38.874938Z","iopub.execute_input":"2024-08-13T22:54:38.875414Z","iopub.status.idle":"2024-08-13T22:54:38.884741Z","shell.execute_reply.started":"2024-08-13T22:54:38.875379Z","shell.execute_reply":"2024-08-13T22:54:38.883503Z"},"trusted":true},"execution_count":null,"outputs":[]}]}